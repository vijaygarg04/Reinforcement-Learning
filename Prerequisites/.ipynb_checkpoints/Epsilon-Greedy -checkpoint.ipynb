{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy.stats import beta\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self,variants,payouts,n_trials,variance=False):\n",
    "        self.variants = variants\n",
    "        if variance:\n",
    "            self.payouts = np.clip(payouts+np.random.normal(0,0.04,size=len(variants)),0,0.2)\n",
    "        else:\n",
    "            self.payouts = payouts\n",
    "        self.n_trials=n_trials\n",
    "        self.total_reward = 0\n",
    "        self.n_k = len(variants)\n",
    "        self.shape = (self.n_k,self.n_trials)\n",
    "    def run(self,agent):\n",
    "        for i in range(self.n_trials):\n",
    "            x_choosen = agent.choose_k()\n",
    "            reward = np.random.binomial(1,p=self.payouts[x_choosen])\n",
    "            agent.reward = reward\n",
    "            agent.update()\n",
    "            self.total_reward +=reward\n",
    "        agent.collect_data()\n",
    "        return self.total_rewards\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSampler:\n",
    "    def __init__(self,env,n_samples=None,n_learning=None,e=0.05):\n",
    "        self.env=env\n",
    "        self.shape=(env.n_k,n_samples)\n",
    "        self.variants=env.variants\n",
    "        self.payouts=env.payouts\n",
    "        self.n_trials = env.n_trials\n",
    "        self.ad_i=np.zeros(env.n_trials)\n",
    "        self.r_i=np.zeros(env.n_trials)\n",
    "        self.thetas=np.zeros(self.n_trials)\n",
    "        self.regret_i=np.zeros(env.n_trials)\n",
    "        self.thetaregret=np.zeros(env.n_trials)\n",
    "        \n",
    "        self.a=np.ones(env.n_k)\n",
    "        self.b=np.ones(env.n_k)\n",
    "        self.theta = np.zeros(env.n_k)\n",
    "        self.data = None\n",
    "        self.reward = 0\n",
    "        self.total_reward = 0\n",
    "        self.k=0\n",
    "        self.i=0\n",
    "        self.n_samples=n_samples\n",
    "        self.n_learning=n_learning\n",
    "        self.e=e\n",
    "        self.ep=np.random.uniform(0,1,size=env.n_trials)\n",
    "        self.exploit=(1-e)\n",
    "        \n",
    "    def collectdata(self):\n",
    "        self.data = pd.DataFrame(dict(ad=self.ad_i,reward=self.r_i,regret = self.regret_i))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSampler(BaseSampler):\n",
    "    def __init__(self,env):\n",
    "        super().__init__(env)\n",
    "    def choose_k(self):\n",
    "        self.k=np.random.choice(self.variants)\n",
    "        return self.k\n",
    "    def update(self):\n",
    "        self.thetaregret[self.i]=np.max(self.theta)-self.theta[self.k]\n",
    "        self.a[self.k]+=self.reward\n",
    "        self.b[self.k]+=1\n",
    "        self.theta=self.a/self.b\n",
    "        self.ad_i[self.i]=self.k\n",
    "        self.r_i[self.i]=self.reward\n",
    "        self.i+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eGreedy(BaseSampler):\n",
    "    def __init__(self,env,n_learning,e):\n",
    "        super().__init(env,n_learning,e)\n",
    "    def choose_k(self):\n",
    "        if(self.i<self.n_learning):\n",
    "            self.k =np.random.choice(self.variants)\n",
    "        else:\n",
    "            self.k = np.argmax(self.theta)\n",
    "        if(self.ep[self.i]>self.exploit):\n",
    "            self.k=np.random.choice(self.variants)\n",
    "        else:\n",
    "            self.k=self.k\n",
    "        return self.k\n",
    "    def update(self):\n",
    "        self.a[self.k]+=self.reward\n",
    "        self.b[self.k]+=1\n",
    "        self.theta= self.a/self.b\n",
    "        self.thetas[self.i]=self.theta[self.k]\n",
    "        self.thetaregret[self.i]=np.max(self.thetas) - self.theta[self.k]\n",
    "        self.ad_i[self.i]=self.k\n",
    "        self.r_i[self.i]=self.reward\n",
    "        self.i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['V00.023',\n",
       " 'V10.03',\n",
       " 'V20.029',\n",
       " 'V30.001',\n",
       " 'V40.05',\n",
       " 'V50.06',\n",
       " 'V60.0234',\n",
       " 'V70.035',\n",
       " 'V80.01',\n",
       " 'V90.11']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_trials = 10000\n",
    "machines = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "payouts = [0.023, 0.03, 0.029, 0.001, 0.05, 0.06, 0.0234, 0.035, 0.01, 0.11]\n",
    "labels = [\"V\" + str(i) + (str(p)) for i, p in zip(machines, payouts)]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Environment' object has no attribute 'total_reward'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-4b5fee2471ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0men0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmachines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpayouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0men0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0men0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-36-b01702ed333e>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, agent)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_reward\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_rewards\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Environment' object has no attribute 'total_reward'"
     ]
    }
   ],
   "source": [
    "en0 = Environment(machines, payouts, n_trials)\n",
    "rs = RandomSampler(env=en0)\n",
    "en0.run(agent=rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
